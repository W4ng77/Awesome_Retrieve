import json
from transformers import AutoTokenizer
from tqdm import tqdm
import os

def chunk_text(text, tokenizer, max_tokens=16):
    tokens = tokenizer(text, return_offsets_mapping=True, truncation=False)
    input_ids = tokens["input_ids"]
    offsets = tokens["offset_mapping"]
    chunks = []
    for i in range(0, len(input_ids), max_tokens):
        if i >= len(offsets):
            break
        start = offsets[i][0]
        end = offsets[min(i + max_tokens - 1, len(offsets) - 1)][1]
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
    return chunks

def load_niah_dataset(file_path, embedding_model="gpt2", chunk_size=16, num_samples=200):
    """
    Load the dataset generated by niah.py and chunk the context.
    """
    tokenizer = AutoTokenizer.from_pretrained(embedding_model)
    
    with open(file_path, "r") as f:
        data = [json.loads(line.strip()) for line in f]

    # Limit to the specified number of samples
    data = data[:num_samples]

    processed_data = []
    chunking_info = []
    
    for idx, example in enumerate(tqdm(data, desc="Processing NIAH dataset")):
        # Extract the query as a single string
        query = example["input"].split("\\n")[-1].strip()

        # Extract context before the last \\n
        context = example["input"].rsplit("\\n", 1)[0]

        # Ensure trailing period
        if context and not context.endswith("."):
            context += "."

        # Extract the answer as a single string
        answer = example["outputs"][0] if isinstance(example["outputs"], list) and example["outputs"] else ""

        context_chunks = chunk_text(context, tokenizer, max_tokens=chunk_size)

        processed_data.append({
            "query": query,     # Single string
            "context": context_chunks,
            "answer": answer    # Single string
        })
        
        chunking_info.append({
            "sample_idx": idx,
            "num_chunks": len(context_chunks),
            "queries": 1  # Since we only keep a single query now
        })
    
    return processed_data, chunking_info
